{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56660d12",
   "metadata": {},
   "source": [
    " ## Case Study #4: Predicting Boston Housing Prices with Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63a14b",
   "metadata": {},
   "source": [
    " ### 1. Upload, explore, clean, and preprocess data for neural network modeling. (Part will not be graded as all questions below have been already done in case study #1).\n",
    "\ta. Create a boston_df data frame by uploading the original data set into Python. Determine and present in this report the data frame dimensions, i.e., number of rows and columns. \n",
    "\n",
    "\t\n",
    "\tb. Display in Python the column titles. If some of them contain two (or more) words, convert them into one-word titles, and present the modified titles in your report. \n",
    "\t\n",
    "\tc. Display in Python column data types. If some of them are listed as “object’, convert them into dummy variables, and provide in your report the modified list of column titles with dummy variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1f623",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2007bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from mord import LogisticIT\n",
    "\n",
    "from dmba import classificationSummary, regressionSummary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db4e60",
   "metadata": {},
   "source": [
    "#### a. Create a boston_df data frame by uploading the original data set into Python. Determine and present in this report the data frame dimensions, i.e., number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce53705e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create boston_df data frame from BostonHousing data set.\n",
    "boston_df=pd.read_csv('BostonHousing.csv')\n",
    "# check the data frame dimensions.\n",
    "boston_df.shape\n",
    "# we have 506 rows and 14 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d685d95",
   "metadata": {},
   "source": [
    "#### b. Display in Python the column titles. If some of them contain two (or more) words, convert them into one-word titles, and present the modified titles in your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bade97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIME', 'ZONE', 'INDUST', 'CHAR RIV', 'NIT OXIDE', 'ROOMS', 'AGE',\n",
       "       'DISTANCE', 'RADIAL', 'TAX', 'ST RATIO', 'LOW STAT', 'MVALUE',\n",
       "       'C MVALUE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the colums heads and the first 10 records.\n",
    "# Check if columns titles contains two or more words.\n",
    "boston_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3369a5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIME', 'ZONE', 'INDUST', 'CHAR_RIV', 'NIT_OXIDE', 'ROOMS', 'AGE',\n",
       "       'DISTANCE', 'RADIAL', 'TAX', 'ST_RATIO', 'LOW_STAT', 'MVALUE',\n",
       "       'C_MVALUE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some of the columns titles contains two words.\n",
    "# Convert the column titles that are not one-word into one-word title.\n",
    "boston_df.columns=[s.strip().replace(' ','_') for s in boston_df]\n",
    "# check the new one-word columns titles.\n",
    "boston_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f414d94",
   "metadata": {},
   "source": [
    "#### c. Display in Python column data types. If some of them are listed as “object’, convert them into dummy variables, and provide in your report the modified list of column titles with dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82472313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIME        float64\n",
       "ZONE         float64\n",
       "INDUST       float64\n",
       "CHAR_RIV      object\n",
       "NIT_OXIDE    float64\n",
       "ROOMS        float64\n",
       "AGE          float64\n",
       "DISTANCE     float64\n",
       "RADIAL         int64\n",
       "TAX            int64\n",
       "ST_RATIO     float64\n",
       "LOW_STAT     float64\n",
       "MVALUE       float64\n",
       "C_MVALUE      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the column data types.\n",
    "# Check if any of the columns is listed as \"object\".\n",
    "boston_df.dtypes\n",
    "# CHAR_RIV, C_MVALUE are object type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efadbf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change CHAR_RIV to type \"category\"\n",
    "boston_df.CHAR_RIV = boston_df.CHAR_RIV.astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12a9365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n"
     ]
    }
   ],
   "source": [
    "# Check CHAR_RIV type\n",
    "boston_df.CHAR_RIV.dtype\n",
    "# OR \n",
    "print(boston_df.CHAR_RIV.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c32d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['N', 'Y'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the categories in CHAR_RIV \n",
    "boston_df.CHAR_RIV.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c200e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n"
     ]
    }
   ],
   "source": [
    "# Change C_MVALUE to type category \n",
    "boston_df.C_MVALUE= boston_df.C_MVALUE.astype('category')\n",
    "# Check C_MVALUE type\n",
    "print(boston_df.C_MVALUE.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0172d06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['No', 'Yes'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the categories in C_MVALUE\n",
    "boston_df.C_MVALUE.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42d3e671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIME', 'ZONE', 'INDUST', 'NIT_OXIDE', 'ROOMS', 'AGE', 'DISTANCE',\n",
       "       'RADIAL', 'TAX', 'ST_RATIO', 'LOW_STAT', 'MVALUE', 'CHAR_RIV_Y',\n",
       "       'C_MVALUE_Yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert category variables in boston_df into dummy variables\n",
    "# Will set drop_first as True to have Yes column and \n",
    "# If the value is 0 in the Y or yes column this will indicate the No or N\n",
    "boston_df=pd.get_dummies(boston_df, prefix_sep='_', drop_first=True)\n",
    "boston_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad00cbb",
   "metadata": {},
   "source": [
    "### 2. Develop a neural network model for Boston Housing and use it for predictions.\n",
    "a. Develop in Python the outcome and predictor variables, partition the data set (60% for training and 40% for validation partitions), display in Python and present in your report the first five records of the training partition. Then, using the StandardScaler() function,\n",
    "develop the scaled predictors for training and validation partitions. Display in Python and provide in your report the first five records of the scaled training partition. Present a brief explanation of what the scaled values mean and how they are calculated.\n",
    "\n",
    "b. Train a neural network model using MLPRegressor() with the scaled training data set and the following parameters: hidden_layer_sizes=9, solver=’lbfgs’, max_iter=10000, and random_state=1. Identify and display in Python the final intercepts and network weights of this model. Provide these intercepts and weights in your report and briefly explain what the values of intercepts in the first and second arrays mean. Also, briefly explain what the values of weights in the first and second arrays mean.\n",
    "\n",
    "c. Using the developed neural network model, make in Python predictions for the outcome variable (MVALUE) using the scaled validation predictors. Based on these predictions, develop and display in Python a table for the first five validation records that contain actual and predicted median prices (MVALUE), and their residuals. Present this table in your report.\n",
    "\n",
    "d. Identify and display in Python the common accuracy measures for training and validation partitions. Provide and compare these accuracy measures in your report and assess a possibility of overfitting. Would you recommend applying this neural network model for predictions? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59641044",
   "metadata": {},
   "source": [
    "#### a. Develop in Python the outcome and predictor variables, partition the data set (60% for training and 40% for validation partitions), display in Python and present in your report the first five records of the training partition. Then, using the StandardScaler() function, develop the scaled predictors for training and validation partitions. Display in Python and provide in your report the first five records of the scaled training partition. Present a brief explanation of what the scaled values mean and how they are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a24d0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictors for Training Partition\n",
      "       CRIME  ZONE  INDUST  NIT_OXIDE  ROOMS   AGE  DISTANCE  RADIAL  TAX  \\\n",
      "452  5.09017   0.0   18.10      0.713  6.297  91.8    2.3682      24  666   \n",
      "346  0.06162   0.0    4.39      0.442  5.898  52.3    8.0136       3  352   \n",
      "295  0.12932   0.0   13.92      0.437  6.678  31.1    5.9604       4  289   \n",
      "88   0.05660   0.0    3.41      0.489  7.007  86.3    3.4217       2  270   \n",
      "322  0.35114   0.0    7.38      0.493  6.041  49.9    4.7211       5  287   \n",
      "\n",
      "     ST_RATIO  LOW_STAT  CHAR_RIV_Y  C_MVALUE_Yes  \n",
      "452      20.2     17.27           0             0  \n",
      "346      18.8     12.67           0             0  \n",
      "295      16.0      6.27           0             0  \n",
      "88       17.8      5.50           0             0  \n",
      "322      19.6      7.70           0             0  \n",
      "\n",
      "Scaled Predictors for Training Partition\n",
      "   CRIME   ZONE  INDUST  NIT_OXIDE  ROOMS    AGE  DISTANCE  RADIAL    TAX  \\\n",
      "0  0.146 -0.482   1.006      1.306  0.083  0.803    -0.688   1.662  1.530   \n",
      "1 -0.419 -0.482  -1.003     -0.955 -0.511 -0.584     1.948  -0.755 -0.337   \n",
      "2 -0.412 -0.482   0.393     -0.996  0.651 -1.328     0.989  -0.640 -0.712   \n",
      "3 -0.420 -0.482  -1.147     -0.563  1.140  0.610    -0.196  -0.871 -0.825   \n",
      "4 -0.387 -0.482  -0.565     -0.529 -0.298 -0.668     0.410  -0.525 -0.724   \n",
      "\n",
      "   ST_RATIO  LOW_STAT  CHAR_RIV_Y  C_MVALUE_Yes  \n",
      "0     0.831     0.560      -0.293         -0.45  \n",
      "1     0.189    -0.061      -0.293         -0.45  \n",
      "2    -1.096    -0.926      -0.293         -0.45  \n",
      "3    -0.270    -1.030      -0.293         -0.45  \n",
      "4     0.556    -0.733      -0.293         -0.45  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Boston outcome and predictors to run neural network\n",
    "# model.\n",
    "outcome = 'MVALUE'\n",
    "predictors = [c for c in boston_df.columns if c != outcome]\n",
    "\n",
    "# Create predictors and outcome variables.  \n",
    "X = boston_df[predictors]\n",
    "y = boston_df[outcome]# Create data partition with training set, 60%(0.6), and \n",
    "# validation set 40%(0.4).\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, \n",
    "                            test_size=0.4, random_state=1)\n",
    "\n",
    "# Display the first 5 records of boston housing training \n",
    "# partition's predictors. \n",
    "print('Predictors for Training Partition')\n",
    "print(train_X.head(5))\n",
    "\n",
    "# Scale input data (predictors) for training  and validation \n",
    "# partitions using StandardScaler().\n",
    "sc_X = StandardScaler()\n",
    "train_X_sc = sc_X.fit_transform(train_X)\n",
    "valid_X_sc = sc_X.transform(valid_X)\n",
    "\n",
    "# Develop a data frame to display scaled predictors for \n",
    "# training partition. Round scaled values to 3 decimals.\n",
    "# Add coloumn titles to data frame.\n",
    "train_X_sc_df = np.round(pd.DataFrame(train_X_sc), decimals=3)                            \n",
    "train_X_sc_df.columns=['CRIME', 'ZONE', 'INDUST', 'NIT_OXIDE', 'ROOMS', 'AGE', 'DISTANCE',\n",
    "       'RADIAL', 'TAX', 'ST_RATIO', 'LOW_STAT', 'CHAR_RIV_Y',\n",
    "       'C_MVALUE_Yes']\n",
    "\n",
    "# Display the first 5 scaled predictors for training partition.\n",
    "print()\n",
    "print('Scaled Predictors for Training Partition')\n",
    "print(train_X_sc_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7dbdee",
   "metadata": {},
   "source": [
    "#### b. Train a neural network model using MLPRegressor() with the scaled training data set and the following parameters: hidden_layer_sizes=9, solver=’lbfgs’, max_iter=10000, and random_state=1. Identify and display in Python the final intercepts and network weights of this model. Provide these intercepts and weights in your report and briefly explain what the values of intercepts in the first and second arrays mean. Also, briefly explain what the values of weights in the first and second arrays mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4084f141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Intercepts for Boston Housing Neural Network Model\n",
      "[array([ 2.26914474,  4.18675342, -1.50466092, -0.95080817,  0.32205673,\n",
      "        0.49141424, -0.81237211,  0.89649621,  2.52709405]), array([-11.97658924])]\n",
      "\n",
      "Network Weights for Boston Housing Neural Network Model\n",
      "[array([[-0.36367279,  0.52687745, -0.00964011, -1.5129948 ,  1.31769872,\n",
      "        -0.41850542, -1.53776369, -0.62480937, -1.90841567],\n",
      "       [ 0.66134206, -1.23924591,  0.28794966,  2.11953871,  1.07434276,\n",
      "         0.43220725, -4.11086338,  0.40334917, -0.08228966],\n",
      "       [ 0.59676784,  0.58331396,  2.48943055,  0.90207757, -1.28237762,\n",
      "         0.42433014, -1.95774694,  0.95234875,  1.23319412],\n",
      "       [-2.20869382, -1.37160062, -0.98568213,  0.36329227, -0.6196563 ,\n",
      "        -0.30782417,  1.05946611,  0.83457418,  2.71468783],\n",
      "       [ 0.16998566, -0.09067205, -1.7444194 ,  0.11677319,  1.80120993,\n",
      "         0.81352311, -0.01783085, -2.03768459,  0.23723986],\n",
      "       [-0.27165058, -2.52251129, -1.29506367,  0.3880619 ,  0.44373225,\n",
      "         0.51115637,  0.8306126 ,  2.20731332,  0.09551513],\n",
      "       [-1.74438324, -1.35593869,  0.74218103, -1.21093971, -0.69583533,\n",
      "        -1.5105213 , -2.02212252,  3.26384001,  2.69895918],\n",
      "       [ 1.78857118,  0.22470477,  0.04506329,  2.44667139, -2.59695682,\n",
      "         3.58357297,  2.76451342,  2.06536825, -2.18559373],\n",
      "       [-2.74560654, -0.75960224, -1.80703499, -0.49290849,  2.18655319,\n",
      "        -1.41363973,  0.08668975, -0.95125975,  1.66200652],\n",
      "       [-0.51989512, -1.10225497, -1.92888649, -0.44066122,  2.16479034,\n",
      "        -1.2403606 ,  1.59544772,  0.83730668, -1.14664323],\n",
      "       [ 0.18569259, -1.05978838, -1.49423995,  0.57938554, -0.87388064,\n",
      "        -0.65901279,  1.50251509, -1.80353844, -0.48843375],\n",
      "       [-0.05494451, -0.8172153 , -0.42448891,  0.43840623,  0.79331301,\n",
      "        -1.1406599 ,  0.14764358,  0.05219393,  0.55909692],\n",
      "       [-1.91183736,  1.16460099, -0.03811763, -2.00838446,  1.32188124,\n",
      "         2.55833196, -0.02954175,  0.43669783, -0.57348116]]), array([[ 3.39687967],\n",
      "       [ 2.13203837],\n",
      "       [-2.33602904],\n",
      "       [-2.86800963],\n",
      "       [ 1.64344769],\n",
      "       [ 1.53156148],\n",
      "       [ 2.48111618],\n",
      "       [ 1.27104678],\n",
      "       [ 2.56836215]])]\n"
     ]
    }
   ],
   "source": [
    "# Use MLPRegressor() function to train neural network model.\n",
    "# Apply: \n",
    "# (a) default input layer with the number of nodes equal \n",
    "#     to number of predictor variables (13); \n",
    "# (b) default single hidden layer with 9 nodes; \n",
    "# (c) default output layer with one outcome variable (Price);\n",
    "# (d) optimization function solver = 'lbfgs', \n",
    "#     which is applied for small data sets for better \n",
    "#     performance and fast convergence. For large data sets, \n",
    "#     apply default solver = 'adam' optimization function;\n",
    "# (e) model is fit with scaled predictors and regular outcome\n",
    "#     in training partition.\n",
    "boston_reg = MLPRegressor(hidden_layer_sizes=(9), \n",
    "                solver='lbfgs', max_iter=10000, random_state=1)\n",
    "boston_reg.fit(train_X_sc, train_y)\n",
    "\n",
    "# Display network structure with the final values of \n",
    "# intercepts (Theta) and weights (W).\n",
    "print('Final Intercepts for Boston Housing Neural Network Model')\n",
    "print(boston_reg.intercepts_)\n",
    "\n",
    "print()\n",
    "print('Network Weights for Boston Housing Neural Network Model')\n",
    "print(boston_reg.coefs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709adca",
   "metadata": {},
   "source": [
    "#### c. Using the developed neural network model, make in Python predictions for the outcome variable (MVALUE) using the scaled validation predictors. Based on these predictions, develop and display in Python a table for the first five validation records that contain actual and predicted median prices (MVALUE), and their residuals. Present this table in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8bc0e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Boston Price for Validation Partition\n",
      "     Actual  Prediction  Residual\n",
      "307    28.2       29.63     -1.43\n",
      "343    23.9       23.49      0.41\n",
      "47     16.6       17.80     -1.20\n",
      "67     22.0       18.73      3.27\n",
      "362    20.8       25.30     -4.50\n"
     ]
    }
   ],
   "source": [
    "# Make 'Price' predictions for validation set using Boston Housing  \n",
    "# neural network model. \n",
    "\n",
    "# Use boston_reg model to predict 'Price' outcome\n",
    "# for validation set.\n",
    "price_pred = np.round(boston_reg.predict(valid_X_sc), decimals=2)\n",
    "\n",
    "# Create data frame to display prediction results for\n",
    "# validation set. \n",
    "price_pred_result = pd.DataFrame({'Actual': valid_y, \n",
    "                'Prediction': price_pred, 'Residual': valid_y-price_pred})\n",
    "\n",
    "print('Predictions for Boston Price for Validation Partition')\n",
    "print(price_pred_result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4793ac37",
   "metadata": {},
   "source": [
    "#### d. Identify and display in Python the common accuracy measures for training and validation partitions. Provide and compare these accuracy measures in your report and assess a possibility of overfitting. Would you recommend applying this neural network model for predictions? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "238efc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Measures for Training Partition for Neural Network\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : -0.0034\n",
      "       Root Mean Squared Error (RMSE) : 1.5617\n",
      "            Mean Absolute Error (MAE) : 1.1368\n",
      "          Mean Percentage Error (MPE) : -0.8274\n",
      "Mean Absolute Percentage Error (MAPE) : 6.0681\n",
      "\n",
      "Accuracy Measures for Validation Partition for Neural Network\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : -0.0912\n",
      "       Root Mean Squared Error (RMSE) : 3.1675\n",
      "            Mean Absolute Error (MAE) : 2.2668\n",
      "          Mean Percentage Error (MPE) : -3.0502\n",
      "Mean Absolute Percentage Error (MAPE) : 11.6748\n"
     ]
    }
   ],
   "source": [
    "# Neural network model accuracy measures for training and\n",
    "# validation partitions. \n",
    "\n",
    "# Identify and display neural network model accuracy measures \n",
    "# for training partition.\n",
    "print('Accuracy Measures for Training Partition for Neural Network')\n",
    "regressionSummary(train_y, boston_reg.predict(train_X_sc))\n",
    "\n",
    "# Identify and display neural network accuracy measures \n",
    "# for validation partition.\n",
    "print()\n",
    "print('Accuracy Measures for Validation Partition for Neural Network')\n",
    "regressionSummary(valid_y, boston_reg.predict(valid_X_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea628fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Partition for the scaled model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# applying confusion materix to asses the errors and accuracy level for the scaled model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Identify and display confusion matrix for training partition. \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Partition for the scaled model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mclassificationSummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboston_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X_sc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Identify and display confusion matrix for validation partition. \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dmba\\metric.py:104\u001b[0m, in \u001b[0;36mclassificationSummary\u001b[1;34m(y_true, y_pred, class_names)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassificationSummary\u001b[39m(y_true, y_pred, class_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\" Print a summary of classification performance\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    Input:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m        class_names (optional): list of class names\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     confusionMatrix \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_true, y_pred)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion Matrix (Accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:307\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfusion_matrix\u001b[39m(\n\u001b[0;32m    223\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    224\u001b[0m ):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:104\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    107\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[1;31mValueError\u001b[0m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "# applying confusion matrix to asses the errors and accuracy level for the scaled model\n",
    "# Identify and display confusion matrix for training partition. \n",
    "print('Training Partition for the scaled model')\n",
    "classificationSummary(train_y, boston_reg.predict(train_X_sc))\n",
    "\n",
    "# Identify and display confusion matrix for validation partition. \n",
    "print()\n",
    "print('Validation Partition for the scaled Model')\n",
    "classificationSummary(valid_y, boston_reg.predict(valid_X_sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb02b52",
   "metadata": {},
   "source": [
    "### 3. Develop an improved neural network model with grid search.\n",
    "a. Use in Python GridSearchCV() function to identify the best number of nodes for the hidden layer in the Boston Housing neural network model. For that, consider the hidden_layer_sizes parameter in a range from 2 to 20. Provide in your report the best score and best parameter value.\n",
    "\n",
    "b. Train an improved neural network model using MLPRegressor() with the scaled training data set and the best identified value of the parameter from the previous question. The rest of the parameters remain the same as in model developed in 2b. Present in your report the final intercepts and network weights of the improved neural network model.\n",
    "\n",
    "c. Identify and display in Python the common accuracy measures for the training and validation partitions with the improved neural network model. Provide and compare these accuracy measures in your report and assess a possibility of overfitting. Would you recommend applying this neural network model for predictions? Briefly explain.\n",
    "\n",
    "d. Present and compare the accuracy measures for the validation partition from the Exhaustive Search model for multiple linear regression in case study #1 and the validation partition for the improved neural network model in this case. Which of the models would you recommend for predictions? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b139ac9",
   "metadata": {},
   "source": [
    "#### a. Use in Python GridSearchCV() function to identify the best number of nodes for the hidden layer in the Boston Housing neural network model. For that, consider the hidden_layer_sizes parameter in a range from 2 to 20. Provide in your report the best score and best parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c51c4256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:0.8084\n",
      "Best parameter:  {'hidden_layer_sizes': 8}\n"
     ]
    }
   ],
   "source": [
    "# Identify grid search parameters. \n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': list(range(2, 20)), \n",
    "}\n",
    "\n",
    "# Utilize GridSearchCV() to identify the best number \n",
    "# of nodes in the hidden layer. \n",
    "gridSearch = GridSearchCV(MLPRegressor(solver='lbfgs', max_iter=10000, random_state=1), \n",
    "                          param_grid, cv=5, n_jobs=-1, return_train_score=True)\n",
    "gridSearch.fit(train_X, train_y)\n",
    "\n",
    "# Display the best score and best parament value.\n",
    "print(f'Best score:{gridSearch.best_score_:.4f}')\n",
    "print('Best parameter: ', gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448925d6",
   "metadata": {},
   "source": [
    "#### b. Train an improved neural network model using MLPRegressor() with the scaled training data set and the best identified value of the parameter from the previous question. The rest of the parameters remain the same as in model developed in 2b. Present in your report the final intercepts and network weights of the improved neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb6532ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Intercepts for Boston Housing Neural Network Model\n",
      "[array([ 0.05058636,  0.27695246,  0.03615363, -0.28178208,  0.17730357,\n",
      "       -0.15748249,  0.18389121, -0.06613618,  0.21517461,  0.14347833]), array([3.08969556])]\n",
      "\n",
      "Network Weights for Boston Housing Neural Network Model\n",
      "[array([[-0.04893654,  0.12993563, -0.29480303, -0.11653212, -0.2083245 ,\n",
      "        -0.24041687, -0.18474998, -0.09108004, -0.06086922,  0.02285407],\n",
      "       [-0.04765472,  0.10923258, -0.15999075,  0.41501449, -0.27872157,\n",
      "         0.10053264, -0.02891929,  0.03461213, -0.14298707, -0.37440681],\n",
      "       [ 0.17719296,  0.27615569, -0.10972874,  0.11714386,  0.2219766 ,\n",
      "         0.2327196 , -0.23547895, -0.27184089, -0.19372822,  0.21809079],\n",
      "       [-0.23687031, -0.04652651,  0.27010435,  0.02041521,  0.11315894,\n",
      "        -0.1087987 ,  0.11130717,  0.19734436, -0.2837778 ,  0.14659305],\n",
      "       [ 0.28836617,  0.14635486, -0.12838849,  0.18639026, -0.2339956 ,\n",
      "        -0.0307249 ,  0.26018929, -0.12171536, -0.11923444, -0.23454294],\n",
      "       [-0.28268919,  0.10546766, -0.16435927, -0.07716523, -0.00496086,\n",
      "        -0.26333739,  0.23062539, -0.20834063,  0.0735263 ,  0.05204141],\n",
      "       [-0.23449059, -0.05068519,  0.11648366, -0.03457942, -0.26541326,\n",
      "         0.02117173,  0.10877693,  0.0087808 ,  0.26776333,  0.03472743],\n",
      "       [ 0.23793805, -0.21379807, -0.21198492,  0.1883544 , -0.06034456,\n",
      "        -0.19735414,  0.26297032, -0.08977957,  0.14912193,  0.12506093],\n",
      "       [ 0.2276967 ,  0.07293533,  0.17816568,  0.34848247, -0.13566692,\n",
      "         0.23360702,  0.57296837,  0.27413785,  0.24278752, -0.57916082],\n",
      "       [-0.22706514,  0.26508478, -0.02686471,  0.0786735 , -0.05417425,\n",
      "        -0.15507506,  0.28627669,  0.04345223, -0.28237526,  0.03324612],\n",
      "       [-0.10233948,  0.01595743,  0.22830337, -0.07594843,  0.24093465,\n",
      "         0.07275505, -0.26900573,  0.25325916,  0.11517346,  0.28299341],\n",
      "       [-0.19323608, -0.21399797,  0.25513272,  0.1167965 , -0.25594994,\n",
      "         0.15065847,  0.14995401,  0.24947729,  0.12542552, -0.22112641],\n",
      "       [-0.28313449, -0.27941547, -0.27801914, -0.14756712,  0.21232527,\n",
      "         0.02290117,  0.03332043,  0.20171157, -0.22087318, -0.13236554]]), array([[ 3.50255086],\n",
      "       [ 3.14457227],\n",
      "       [ 3.0791651 ],\n",
      "       [-0.53200368],\n",
      "       [-0.38839374],\n",
      "       [ 3.11987483],\n",
      "       [-0.45088437],\n",
      "       [ 3.63622424],\n",
      "       [ 3.52334577],\n",
      "       [ 3.04247665]])]\n"
     ]
    }
   ],
   "source": [
    "# Use MLPRegressor() function to train the improved neural network model\n",
    "# based on grid search results. \n",
    "\n",
    "# Apply: \n",
    "# (a) default input layer with the number of nodes equal \n",
    "#     to number of predictor variables (13); \n",
    "# (b) single hidden layer with 10 nodes based on grid search; \n",
    "# (c) default output layer with the number nodes equal\n",
    "#     to number of classes in outcome variable (3);\n",
    "# (d) 'logistic' activation function;\n",
    "# (e) solver = 'lbfgs', which is applied for small data \n",
    "#     sets for better performance and fast convergence. \n",
    "#     For large data sets, apply default solver = 'adam'. \n",
    "boston_clf_imp = MLPRegressor(hidden_layer_sizes=(10), max_iter=10000,\n",
    "                activation='logistic', solver='lbfgs', random_state=1)\n",
    "boston_clf_imp.fit(train_X, train_y)\n",
    "\n",
    "# Display network structure with the final values of \n",
    "# intercepts (Theta) and weights (W).\n",
    "print('Final Intercepts for Boston Housing Neural Network Model')\n",
    "print(boston_clf_imp.intercepts_)\n",
    "\n",
    "print()\n",
    "print('Network Weights for Boston Housing Neural Network Model')\n",
    "print(boston_clf_imp.coefs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0ea8c",
   "metadata": {},
   "source": [
    "#### c. Identify and display in Python the common accuracy measures for the training and validation partitions with the improved neural network model. Provide and compare these accuracy measures in your report and assess a possibility of overfitting. Would you recommend applying this neural network model for predictions? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40673a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Measures for Training Partition for Neural Network\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 0.0000\n",
      "       Root Mean Squared Error (RMSE) : 8.9460\n",
      "            Mean Absolute Error (MAE) : 6.5493\n",
      "          Mean Percentage Error (MPE) : -18.8601\n",
      "Mean Absolute Percentage Error (MAPE) : 37.1695\n",
      "\n",
      "Accuracy Measures for Validation Partition for Neural Network\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 1.0476\n",
      "       Root Mean Squared Error (RMSE) : 9.5609\n",
      "            Mean Absolute Error (MAE) : 6.6363\n",
      "          Mean Percentage Error (MPE) : -12.0971\n",
      "Mean Absolute Percentage Error (MAPE) : 32.6626\n"
     ]
    }
   ],
   "source": [
    "# Neural network model accuracy measures for training and\n",
    "# validation partitions. \n",
    "\n",
    "# Identify and display neural network model accuracy measures \n",
    "# for training partition.\n",
    "print('Accuracy Measures for Training Partition for Neural Network')\n",
    "regressionSummary(train_y, boston_clf_imp.predict(train_X))\n",
    "\n",
    "# Identify and display neural network accuracy measures \n",
    "# for validation partition.\n",
    "print()\n",
    "print('Accuracy Measures for Validation Partition for Neural Network')\n",
    "regressionSummary(valid_y, boston_clf_imp.predict(valid_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2454448",
   "metadata": {},
   "source": [
    "#### d. Present and compare the accuracy measures for the validation partition from the Exhaustive Search model for multiple linear regression in case study #1 and the validation partition for the improved neural network model in this case. Which of the models would you recommend for predictions? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47ef7ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction for Validation Set Using Exhaustive Search\n",
      "     Actual  Predicted  Residual\n",
      "307    28.2      25.24      2.96\n",
      "343    23.9      22.78      1.12\n",
      "47     16.6      18.17     -1.57\n",
      "67     22.0      21.86      0.14\n",
      "362    20.8      18.93      1.87\n",
      "132    23.0      19.58      3.42\n",
      "292    27.9      25.25      2.65\n",
      "31     14.5      18.06     -3.56\n",
      "218    21.5      22.49     -0.99\n",
      "90     22.6      23.28     -0.68\n",
      "\n",
      "Accuracy Measures for Validation Set Using Exhaustive Search\n",
      "\n",
      "Regression statistics\n",
      "\n",
      "                      Mean Error (ME) : 0.4505\n",
      "       Root Mean Squared Error (RMSE) : 3.8674\n",
      "            Mean Absolute Error (MAE) : 2.7724\n",
      "          Mean Percentage Error (MPE) : -2.1963\n",
      "Mean Absolute Percentage Error (MAPE) : 13.3441\n"
     ]
    }
   ],
   "source": [
    "# Develop the multiple linear regression model based on the Exhaustive Search results.\n",
    "\n",
    "# Identify predictors and outcome of the regression model.\n",
    "predictors_ex = ['CHAR_RIV_Y', 'CRIME', 'C_MVALUE_Yes', \n",
    "                 'DISTANCE', 'INDUST','LOW_STAT', 'NIT_OXIDE', \n",
    "                 'RADIAL', 'ROOMS', 'ST_RATIO', 'TAX']\n",
    "outcome = 'MVALUE'\n",
    "\n",
    "# Identify X and y variables for regression and partition data\n",
    "# using 60% of records for training and 40% for validation \n",
    "# (test_size=0.4). \n",
    "X = boston_df[predictors_ex]\n",
    "y = boston_df[outcome]\n",
    "train_X_ex, valid_X_ex, train_y_ex, valid_y_ex = \\\n",
    "          train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "# Create multiple linear regression model using X and y.\n",
    "boston_ex = LinearRegression()\n",
    "boston_ex.fit(train_X_ex, train_y_ex)\n",
    "# Use predict() function to score (make) predictions \n",
    "# for validation set and measure their accuracy using\n",
    "# Exhaustive Search algorithm.\n",
    "boston_ex_pred = boston_ex.predict(valid_X_ex)\n",
    "\n",
    "# Develop and display data frame with actual values of Price,\n",
    "# scoring (predicted) results, and residuals.\n",
    "# Use round() function to round vlaues in data frame to \n",
    "# 2 decimals. \n",
    "result = round(pd.DataFrame({'Actual': valid_y_ex,'Predicted': boston_ex_pred, \n",
    "                       'Residual': valid_y_ex - boston_ex_pred}), 2)\n",
    "print()\n",
    "print('Prediction for Validation Set Using Exhaustive Search') \n",
    "print(result.head(10))\n",
    "\n",
    "# Display common accuracy measures for validation set.\n",
    "print()\n",
    "print('Accuracy Measures for Validation Set Using Exhaustive Search')\n",
    "regressionSummary(valid_y_ex, boston_ex_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1788dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d093014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce931ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6b43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f61b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8e49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24777eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cca43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31097d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
